<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Automatic Class Characteristic Recognition in Shoe Tread Images</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jayden Stack, Rick Stone, Colton Fales, and Susan VanderPlas" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/csafe.css" type="text/css" />
    <link rel="stylesheet" href="css/csafe-fonts.css" type="text/css" />
    <link rel="stylesheet" href="css/this-presentation.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Automatic Class Characteristic Recognition in Shoe Tread Images
### Jayden Stack, Rick Stone, Colton Fales, and Susan VanderPlas

---






class: inverse-blue
## Outline
&lt;br/&gt;&lt;br/&gt;

.large[

- Motivation and Project Goals

- Scanner Images

- Statistical Modeling

- Future Work

]

---
class:inverse-red,middle,center

# Motivation

---
class:primary-red
## Footwear Reference Populations

### What is the probability of a coincidental match?

1. Define the comparison population
2. Sample from the comparison population ( `\(N\)` shoes)
3. Identify similar shoes from the comparison population ( `\(S\)` of the `\(N\)` shoes)
4. Estimate the probability of a coincidental match ( `\(\hat{p} = S/N\)` )

???

Statistical assessment of footwear has been difficult historically, at least in part because we lack data on the reference population. If we wanted to estimate the probability of a coincidental match in footwear, like we do with DNA analysis, we would need to do the following: ...

The issue is that we don't have any way to get data on the comparison population in footwear. There's no easy way to gather that data at the moment, and even if we could gather it, we don't have any way to identify similar shoes based on features. 

Something like 95% of footwear evidence in the US is based on class characteristic matches, so this is not a trivial problem - we need more information about class characteristics in local populations.

---
class: primary-red
## Objectives

.pull-left-40[
![Shoe Scanner Outside Setup](Images/2021-June-1.png)
].pull-right-60[
- Develop a shoe scanner that can gather population footwear data automatically

- Use the scanner to gather images of shoe soles from the population

- Identify features in the shoe sole patterns that make for a relatively unique description of the pattern

- Use pattern descriptions to characterize the types of shoes worn by a population

- Eventually, maybe use this data to get a random match probability for forensic analysis
]

???

The goal of this project, in forensic space, is to develop a method for sampling and assessing which shoes are common in a local population. In order to do much of anything probabilistically with forensic shoe evidence, we need to understand the local population of potential suspects, but gathering that data is not something that has been feasible up until this point. The amazing engineering team at Iowa State has built a shoe scanner that serves kind of like a field camera in the wild. As people walk over the scanner, it takes pictures of the bottom of their shoes. At the same time, we are also working hard to ensure that the shoes are the only thing that is captured in the photo. 

The goal here is to then take the pictures we gather, identify features that describe the shoe tread pattern, and work with a basis of those pattern features to describe the local population. This may eventually help us calculate a random match probability between evidence at a crime scene and the local population.


---
class: primary-red
## The Scanner
&lt;img src="Images/2021-May-1.png" width = "30%"/&gt;&lt;img src="Images/2021-August-2.png" width = "70%"/&gt;

The scanner is on display at the CSAFE booth!


---
class: inverse-cyan,middle,center
# Scanner Images


---
class: primary-cyan
## Indoor Testing

&lt;img src="Images/pic1.jpg" width = "40%"/&gt;
&lt;img src="Images/pic2.jpg" width = "40%"/&gt;
&lt;img src="Images/pic4.jpg" width = "40%"/&gt;
&lt;img src="Images/pic5.jpg" width = "40%"/&gt;


???

The scanner is able to detect the pressure being put on top of it, and then automatically take a photo of what is on top. As you might be able to notice, these images might have a little bit of reflection from either the lights in the room, or the camera itself. 

---
class: primary-cyan
## Image Quality Comparisons

- Before this scanner was built, there was no way to easily obtain raw images or prints of shoe soles in the general population
    - Studies were small scale and required lots of manual labor to gather a small amount of data (Benedict, et al., 2014)

- Many computational studies use marketing-quality images scraped from the web 

- The difference is substantial!

&lt;img src="Images/image1.jpg" width = "30%"/&gt;
&lt;img src="Images/image2.jpg" width = "30%"/&gt;
&lt;img src="Images/image3.jpg" width = "30%"/&gt;
&lt;img src="Images/GOPR8318.JPG" width = "30%"/&gt;
&lt;img src="Images/GOPR8320.JPG" width = "30%"/&gt;
&lt;img src="Images/GOPR8341.JPG" width = "30%"/&gt;

???

As a preface to the brilliant work done by the engineers, all of the previous work that had been done on this project had been completed on images Zappos website via web scraping. While the details of the images are very clear, it is important to note that in nature, this is almost never the case. Everyone has different wear on the treads of their shoes which will make it more difficult to identify features on the bottoms of those shoes. Now that the scanner has been built, our data will now more likely mirror that of shoe images found in a forensic space. In the grander scheme of things, we now have data and the ability to collect data that can be utilized in court to show feature tendency in local populations. in Please check out the CSAFE booth where the scanner can be demonstrated and Dr. Rick Stone can answers questions that you may have. 

---
class: primary-cyan
## Comparison Data

.pull-left[
#### Online Shopping Data

- Clearer images

- Huge amounts of data available

- Not representative of a local community

- Not representative of data quality from the scanner
].pull-right[
#### Shoe Scanner data

- Tread worn down, mud/dirt, glare from the scanner, outdoor lighting conditions

- Less data available     
`\(&lt;300\)` im/wk over the summer

- Representative of the local community

- **Plan**: Combine both datasets! Update a model trained on online shopping data with messier scanner data.
]

???

When we started the project, we only had the online shopping images; now we have scanner images as well, So the plan is to train a model on the online shopping data, and then tweak/update the model weights to account for reduced data quality with the shoe scanner data once we get enough data out of the scanner to label and fit a model. 

Some of you may be wondering why we're working with specific features of the shoe pattern instead of trying to identify the specific shoe models. There are many reasons -- first, there's no database of shoe patterns out there that match to specific shoe models. Different year models actually have different patterns, and some shoe sole patterns are shared across models. In addition, there are tons of knockoffs, and shared patterns across different brands of similar types of shoes. So in my opinion, at least, it's more productive to work with tread features and not worry about identifying specific brand/style/models of shoes. 

---
class:inverse-green,center,middle
# Statistical Modeling

---
class:primary-green
## Statistical Modeling

- **Transfer Learning**
    - Take a model trained for something else and tune it to work with a new type of data
    - Leverages past work on computer vision
    - Customizes the work for a specific task (shoe pattern recognition)


- 2 stages of transfer in this project:
    1. Use computational models for image recognition to recognize features in marketing-quality shoe images
        - Initial data: Photographs
        - Initial features: identify cats, dogs, etc.
        - Target data: Photographs of shoes
        - Target features: identify tread pattern elements like circles, chevrons, logos, sizes, and brand
    2. Use shoe feature recognition model to identify features in "messy" scanner images

???

The other half of the battle is being able to automatically identify which class characteristics are coming the images. We have been able to use machine learning techniques to fit a model but with flaws. The major flaw is how to know when the feature in question is actually present in the image. Most of us would be able to detect when there might be a circle in an image, but the model that has been trained is able to find circles where where we otherwise would not see them. 

---
class:primary-green
## Challenges

- It's hard for us to define a circle

![Heatmap of Squircle](Images/heatmap-quad-4-dc-pure-se-navy_product_7270757_color_9.png)
![Heatmap of Adidas text](Images/heatmap-test_image.png)

---
class:primary-green
## Challenges

- It's hard for computer vision models to identify a circle

![Heatmap of Spiral text](Images/heatmap-text-2-seychelles-slow-down-blush-metallic_product_9017725_color_34700.png)

Conclusion: Things that are relatively easy for humans are hard for computers. 

???

While we want to use human-friendly descriptors, it's very difficult for us to train computer models in ways that match human use of language precisely, especially for patterns like those on shoe soles.

---
class:inverse-grey,middle,center

# Future Work

---
class:primary-grey
## Moving forward 

- We are currently working on utilizing slightly more advanced machine learning techniques to pin point labeled class characteristics on images. 

- The new model will be able to create a box around the feature in question and correctly predict an "objectness" score of that feature. 

    - That is, this feature inside this box is 99.987% a circle. 
    
    - We would be able to place a baseline measurement of how high of the percentage needs to be.
    
???

Using state-of-the-art machine learning techniques, the plan is to keep trying to fit a model that will give us what we want. After reading literature of the best methods of object-detection, 



---
class:primary-grey
## Acknowledgements

Work on this project was supported by:

- National Institutes of Justice (2019-MU-MU-4096)

- Center for Statistical Applications in Forensic Evidence (CSAFE) through
Cooperative Agreements 70NANB15H176 and 70NANB20H019 between NIST and Iowa State University, which includes activities
carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia
University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.

---
class:primary-grey
## Questions

- We are looking for collaborations with practitioners

- See the scanner prototype at the CSAFE booth

- Email us with any additional questions or to discuss collaboration!
    - Jayden Stack: jstack9@huskers.unl.edu 
    - Dr. Vanderplas: susan.vanderplas@unl.edu



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
